##StripColumnsFromCSV
cut -d "," -f6-7 20M_Hashes_copy.csv > 20M_col.6.7_Hashes.csv

##Pick Random lines from file:
shuf -n N input > output


##Dedupe lines in file 
sort myfile.txt | uniq >> deduped_myfile.txt

##Remove dupes from 2nd column
awk  -F $','' '!a[$2]++' ALL_other.csv 


##Combine two CSV's
sed 1d file2.csv > file2noheader.csv
cat file1.csv file2noheader.csv > outputfile.csv

##Insert Headers to file
ex -sc '1i|MY New Text' -cx Insert_line.txt
ex -sc '1i|"Input Hash","URL","Classified Sentence","Name","ID","Path","ID Path","Source File"' -cx Insert_line.txt  

##"Input Hash","URL","Classified Sentence","Name","ID","Path","ID Path","Source File"

##Remove last line of file
tail -n 1 "$file" | wc -c | xargs -I {} truncate "$file" -s -{}

#Remove and display to stdout
tail -n 1 "$file" | tee >(wc -c | xargs -I {} truncate "$file" -s -{})

##For OSX, Remove Last Line
cat "FILE" | tail -r | tail -n +2 | tail -r

split -l 200000 filename

##Remove dupes from 2nd column
awk  -F $','' '!a[$2]++' ALL_other.csv 

##Run command after network interface is brought up
auto eth0
    iface eth0 inet dhcp
    post-up /usr/local/sbin/my-custom-script

## Nohup
nohup find -size +100k > log.txt &


##Get file encodings
file -I {filename}

##Combine json files from folder 
find data/ -name '*.json' -exec cat {} \; > uber.json
